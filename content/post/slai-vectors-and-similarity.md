---
title: "Vectors in AI: Definition, use cases, and practical examples"
type: "post"
date: 2025-10-28T08:05:32+07:00
description: "In todayâ€™s post, weâ€™re going to talk about vectors and how theyâ€™re used in artificial intelligence."
keywords: ["vectors", "ai", "vectors use cases"]
categories: ["AI"]
tags: ["artificial-intelligence"]
image: "/common/ai.png"
---

In machine learning, vector is an array of numbers use to reflect the features.
The target is transforming complex data structure like text, images, audios to numbers that a computer program can compare.

In math or physic, vector is a quantity that has both size (magnitude) and direction.

## Problem & Solution

We are going to build a chatbot system to enhance user experience. Let's take a look at this use case.

The food store serves the following foods and drinks:

1. Bread and fried eggs
2. Pho Noodle (Vietnamese noodle)
3. Coffee
4. Tea
5. Coca
6. Bun Noodle (Vietnamese noodle)

A bot ask user: What do you like for this morning?
User respond: I want something soupy

So what should the system respond in this case, if you are a server(waiter/waitress), you can easily suggest Pho Noodle, Bun Noodle. But if it is a computer program, how to do that?

This is where we can use vector to solve this problem by using semantic search and features not a text matching search algorithm.

## How do vector work in this scenario ?

Example:

The sentence â€œTÃ´i thÃ­ch Äƒn phá»Ÿâ€ is converted into a 384-dimensional vector.
The sentence â€œTÃ´i mÃª phá»Ÿ bÃ² láº¯mâ€ is converted into another 384-dimensional vector.

â†’ If the two sentences have similar meaning, their vectors will be positioned close to each other in vector space.

To formalize the notion of â€œcloseness,â€ we use similarity or distance metrics. Common metrics include:

- dot product
- cosine similarity
- Euclidean distance

So that to allow the computer program can compare distance between 2 vectors (use search and the data), all data must be transformed into multi dimensional vector. That technique is called "Embedding".

### Embedding

An embedding is a technique for representing objects (such as text or images) as numerical vectors in a multi-dimensional space. This allows a machine to capture and reason about their meaning and relationships. These representations are learned automatically during training, enabling more efficient processing of complex information in tasks such as semantic search, classification, and recommendation.

#### Why does embedding-based search work?

An embedding maps a sentence or passage of text into a high-dimensional vector (e.g. 384, 768, or 1536 dimensions, depending on the model).

These dimensions are learned in such a way that:

- Texts with similar meaning produce vectors that are close to each other.
- Texts with different meaning produce vectors that are far apart.
- This property is known as â€œsemantic similarity in high-dimensional space.â€

**Example:**

Query: "how to fix iPhone battery draining fast"
Document A: "troubleshooting steps to reduce iPhone battery drain"
Document B: "discount iPhone fast chargers available now"

A standard keyword search may return both documents, since they both contain terms like "iPhone" and "battery."
However, an embedding-based search can identify that Document A is semantically relevant to the userâ€™s intent (diagnosing and fixing battery drain), whereas Document B is focused on purchasing accessories, which does not address the underlying issue.

=> This is why semantic search is often more effective than pure keyword matching.

### Dot product (tÃ­ch vÃ´ hÆ°á»›ng)

Assume we have two vectors of the same dimension:

a = [aâ‚, aâ‚‚, ..., aâ‚™]
b = [bâ‚, bâ‚‚, ..., bâ‚™]

The dot product is defined as:
a Â· b = aâ‚Ã—bâ‚ + aâ‚‚Ã—bâ‚‚ + ... + aâ‚™Ã—bâ‚™

Intuition:

- If a and b point in a similar direction, the dot product is large (positive).
- If a and b point in opposite directions, the dot product is negative.
- If they are orthogonal (unrelated), the dot product is approximately 0.

### Cosine similarity ( Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine )

> Báº£n cháº¥t lÃ  Ä‘o gÃ³c giá»¯a 2 vector chá»© khÃ´ng quan tÃ¢m Ä‘á»™ lá»›n

Cosine similarity measures the angle between two vectors, rather than their absolute magnitudes.

The formula is:

cosine_sim(a, b) =
(a Â· b) / (â€–aâ€– Ã— â€–bâ€–)

Where:

- a Â· b is the dot product
- â€–aâ€– is the length (magnitude) of vector a, defined as sqrt(aâ‚Â² + aâ‚‚Â² + ... + aâ‚™Â²)

Interpretation:

- 1 â†’ the two vectors point in exactly the same direction (high semantic similarity)
- 0 â†’ the vectors are unrelated / orthogonal
- -1 â†’ the two vectors point in opposite directions (opposite meaning)

Because cosine similarity focuses only on the angle between vectors, it is widely used to measure semantic similarity between two sentences after they have been embedded.

## Basic text representation

Tokenization is the process of splitting a sentence into smaller units called tokens.

In the simplest form, we can split by whitespace:
"TÃ´i thÃ­ch Äƒn phá»Ÿ bÃ²" â†’ ["TÃ´i", "thÃ­ch", "Äƒn", "phá»Ÿ", "bÃ²"]

Modern language models, however, use more advanced tokenizers that can break words into subword units. This allows the model to handle rare or unseen words by decomposing them into smaller meaningful parts.

### Stop-word removal

Stop words are very common words that typically carry low discriminative value, such as:

English: "the", "is", "and", "of"

Vietnamese: "lÃ ", "vÃ ", "cá»§a", "thÃ¬", "lÃºc Ä‘Ã³", "cÃ¡i nÃ y", ...

In traditional approaches like Bag-of-Words (BoW) and TF-IDF, these words are often removed to reduce noise.

Note: In modern embedding-based systems, manual stop-word removal is usually unnecessary because the model implicitly down-weights those words.

### Bag-of-Words (BoW)

Core idea:

Count how many times each word appears.

Ignore word order entirely.

Consider the two sentences:

- "tÃ´i thÃ­ch phá»Ÿ bÃ²"
- "phá»Ÿ bÃ² ngon, tÃ´i ráº¥t thÃ­ch"

Shared vocabulary = ["tÃ´i","thÃ­ch","phá»Ÿ","bÃ²","ngon","ráº¥t"]

Their BoW representations (following the vocabulary order above) are:

- Sentence 1 â†’ [1,1,1,1,0,0]
- Sentence 2 â†’ [1,1,1,1,1,1]

Advantages:

- Simple and easy to compute.

Limitations:

- It does not capture context (for example, â€œbÃ²â€ could mean â€œbeefâ€ or â€œcowâ€).
- It does not capture synonymy (â€œngonâ€ and â€œtuyá»‡t vá»iâ€ express similar sentiment but are treated as different words).

### TF-IDF

**TF-IDF stands for Term Frequency â€“ Inverse Document Frequency.**

Intuition:

- Term Frequency (TF): how often a term appears in a given document.
- Inverse Document Frequency (IDF): how rare that term is across the entire collection of documents.

In practice:

- High TF â†’ the term is important for that specific document.
- High IDF â†’ the term is rare elsewhere, so it helps distinguish that document from others.

For example:

- A term like "phá»Ÿ" may appear frequently only in a few documents â†’ high IDF â†’ considered important.
- A term like "vÃ " appears in almost every document â†’ low IDF â†’ low value.

**Compared to BoW, TF-IDF improves relevance by down-weighting extremely common words.**

However, TF-IDF is still fundamentally keyword matching. It does not truly understand meaning or semantic relationships.

## Keyword Search vs. Semantic Search

### Keyword search (BoW / TF-IDF / BM25):

- Keyword-based retrieval relies on explicit term matching.
- It performs well when the query requires an exact phrase or technical term, such as debugging an error message: "TypeError: cannot read property 'map' of undefined".
- However, it becomes less effective when the user expresses the same intent using different wording, for example: â€œnhÃ  hÃ ng chayâ€ (â€œvegetarian restaurantâ€) vs. â€œquÃ¡n cÆ¡m chay lÃ nh máº¡nhâ€ (â€œhealthy vegetarian rice placeâ€).

### Semantic search (Embedding + Cosine similarity):

- Semantic search relies on meaning rather than exact word overlap.
- It can still retrieve relevant documents even when the wording in the query does not match the wording in the document.
- This approach is especially effective for natural-language questions, FAQs, internal chatbots, and RAG pipelines.

=> In a Retrieval-Augmented Generation (RAG) system, semantic search is typically used to retrieve the most relevant context for the language model.

## Evaluation Metrics

Suppose you have an information retrieval system. A user submits a query, and the system returns a ranked list of documents (top K). You now want to evaluate: â€œDid we retrieve the relevant documents?â€

The standard metrics are precision, recall, and F1.

### Precision

Precision answers the question:
Of all the documents the system returned, how many were actually relevant?

**Formula:**

Precision =
(number of relevant documents retrieved)
/
(total number of documents retrieved)

**Example:**

The system returns 5 documents, and 3 of them are truly relevant.
â†’ Precision = 3 / 5 = 0.6 (60%)

> High precision means the system returns very little noise.

### Recall

Recall answers the question:
Out of all the relevant documents that exist, how many did the system manage to retrieve?

Formula:

Recall = (number of relevant documents retrieved) / (total number of relevant documents in the entire dataset)

**Example:**

There are 6 relevant documents in the entire corpus.
Your system returns 3 of them.
â†’ Recall = 3 / 6 = 0.5 (50%)

High recall means the system is not missing important documents.

**Precision vs. Recall**

High precision with low recall means the system is very strict: it only returns what itâ€™s confident about. The results are clean, but it may miss many relevant documents.

High recall with low precision means the system returns a lot: it captures most of the relevant material, but it also brings in a lot of irrelevant content.

Which one matters more depends on the use case:

For sensitive assistants (e.g. legal or medical chatbots), high precision is critical, because returning wrong or misleading content is dangerous.

For discovery or research pipelines where humans will review results later, high recall is often preferred, because missing relevant evidence is costly.

### F1-score

The F1-score is the harmonic mean of precision and recall. It balances both.

Formula:

F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)

A high F1-score indicates that the system is both accurate (high precision) and comprehensive (high recall).

## Examples

### Tokenization

```python
from transformers import GPT2Tokenizer

def observe_tokenization(text):
  tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
  tokens = tokenizer.tokenize(text)
  ids = tokenizer.encode(text)

  print("Tokens:", tokens)
  print("Token IDs:", ids)
  print("Decoded back:", tokenizer.decode(ids))
```

### Similarity

Imagine each number in the vector is a feature of an animal.
For example:

- Feature 1: number of legs
- Feature 2: how â€œfriendly/close to humansâ€ it is (1 = not very, 3 = more familiar, 10 = an elephantâ€¦ which is kind of its own thing ðŸ˜)
- Feature 3: relative size / body mass

(This is just for illustration, not real scientific data.)

Now we want to ask: which animals are the most similar to each other?

```python
cat       = [4, 1, 3]
dog       = [4, 1, 4]
elephant  = [4, 3, 10]
```

```python
# Aâ‹…B=a1Ã—b1+a2Ã—b2+a3Ã—b3
cat = [4, 1, 3]
dog = [4, 1, 4]

dot_product_cat_and_dog = 4*4 + 1 * 1 + 3*4 = 29

cat = [4, 1, 3]
elephant = [4, 3, 10]

dot_product_cat_and_elephant = 4*4 + 1 * 3 + 3*10 = 49

dog = [4, 1, 4]
elephant = [4, 3, 10]

dot_product_dog_and_elephant = 4*4 + 1 * 3 + 4*10 = 59

# âˆ¥Aâˆ¥=a12+a22+a32
â€–catâ€– vá»›i cat = [4, 1, 3]
â€–catâ€– = âˆš26
â€–dogâ€– vá»›i dog = [4, 1, 4]
â€–dogâ€– = âˆš33
â€–elephantâ€– vá»›i elephant = [4, 3, 10]
â€–elephantâ€– = âˆš125

cos(Î¸)= (A*B)â€‹ / (â€–Aâ€–*â€–Bâ€–)

Cosine(cat, dog) = 0.9900
Cosine(cat, elephant) = 0.8595
Cosine(dog,elephant) = 0.9186
```

cosine_similarity expects the input to be in the shape (n_samples, n_features), meaning a 2D matrix.

cat.reshape(1, -1) = 1 sample = the cat
dog.reshape(1, -1) = 1 sample = the dog

If you just pass a 1D vector like [4, 1, 3] directly, it might throw an error.

## Terminologies

> Scalar: something that has size (magnitude) but no direction: length (Ä‘á»™ dÃ i), area (diá»‡n tÃ­ch), volume (thá»ƒ tÃ­ch), speed (tá»‘c Ä‘á»™), mass (khá»‘i lÆ°á»£ng), density (máº­t Ä‘á»™), pressure (Ã¡p suáº¥t), temperature (nhiá»‡t Ä‘á»™), energy (nÄƒng lÆ°á»£ng), entropy, work (cÃ´ng), power (cÃ´ng suáº¥t)  
> Vector: a quantity that has both size (magnitude) and direction: displacement ( Ä‘á»™ dá»i ), velocity (váº­n tá»‘c), acceleration (gia tá»‘c), momentum (Ä‘á»™ng lÆ°á»£ng), force(lá»±c), lift(lá»±c nÃ¢ng), drag(lá»±c cáº£n), thrust(lá»±c Ä‘áº©y), weight(trá»ng lÆ°á»£ng)

## Images

> Mass, Weight

![Mass, Weight](/ai/001.png)

> Energy, Work, Power

![Energy, Work, Power](/ai/002.png)
